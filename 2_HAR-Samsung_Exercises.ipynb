{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2\n",
    "\n",
    "## Overview\n",
    "During this session, you will walk through an exercise of machine learning based on human activity data (e.g. walking, resting, etc.) captured using a smartphone's sensors. You will use unsupervised learning to start, and complement it with a supervised learning algorithm for description purposes.\n",
    "\n",
    "- You will be provided with some context and the necessary data to perform the exercise. \n",
    "- You will perform an analysis to learn how to discriminate between different types of activities.\n",
    "- You will create a descriptive model using decision trees.\n",
    "\n",
    "## Human Activity Recognition using Smartphones\n",
    "Human Activity Recognition (HAR) strives to identify the actions performed by a person from a series of observations of them and their environmental conditions. Some approaches have relied on the use of dedicated motion sensors in different parts of the body, which achieve good classification accuracy. However, these sensors are typically uncomfortable for the common user, so smartphones with their embedded built-in sensors, emerged as an alternative to gather context information about people's actions.\n",
    "\n",
    "The dataset used in this example was built using inertial data obtained from smartphone accelerometers and gyroscopes. The dataset came from the recordings from 30 subjects doing the following human activities:\n",
    "1. Walking\n",
    "2. Walking Upstairs\n",
    "3. Walking Downstairs\n",
    "4. Sitting\n",
    "5. Standing\n",
    "6. Laying\n",
    "\n",
    "Applications derived from analyzing this dataset could involve activity logging to provide a summary on a person's lifestyle. The aim of this exercise is to identify features that help discriminate the aforementioned activities. To attain this, Hierarchical Clustering Analysis (HCA), Principal Component Analysis (PCA) and classification trees will be applied.\n",
    "\n",
    "\n",
    "**References:**\n",
    "\n",
    "Anguita, D., Ghio, A., Oneto, L., Parra, X., & Reyes-Ortiz, J. L. (2013, April). *A Public Domain Dataset for Human Activity Recognition using Smartphones*. In ESANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download\n",
    "If the dataset is not already on the working directory, it can be sourced from the University of California, Irvine (UCI) Machine Learning Repository https://archive.ics.uci.edu/ml/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, requests, zipfile, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "import pydotplus\n",
    "\n",
    "if not os.path.isdir('./Data/UCI HAR Dataset/'):\n",
    "    HAR_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip'\n",
    "    req = requests.get(HAR_URL)\n",
    "    compressed = zipfile.ZipFile(io.BytesIO(req.content))\n",
    "    compressed.extractall('./Data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "The dataset is composed of 561 features recorded from 10,299 observations, and is divided into:\n",
    "- Training dataset: 7352 observations about 21 subjects\n",
    "- Testing dataset (not considered): 2947 observations about 9 subjects, which is not present in the training dataset\n",
    "Review the class distribution to understand how balanced the dataset is. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_key_value_pairs(filename):\n",
    "    '''Returns the key value pairs, separated by whitespaces,\n",
    "    that are stored in a file.'''\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            key, value = line.strip().split(' ')\n",
    "            yield key, value\n",
    "\n",
    "\n",
    "def HCA(data, class_labels, class_color, method='complete', metric='euclidean'):\n",
    "    '''Performs Hierarchical Clustering Analysis on data, an\n",
    "    n by p feature matrix, to plot the corresponding dendrogram\n",
    "    and return the linkage matrix.'''\n",
    "    \n",
    "    merge_dist = linkage(data, method=method, metric=metric)\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.title('Hierarchical Clustering Dendrogram', size=16)\n",
    "    plt.xlabel('sample index')\n",
    "    plt.ylabel(metric+' distance')\n",
    "    dendrogram(\n",
    "        merge_dist,\n",
    "        leaf_rotation=90.,\n",
    "        leaf_font_size=8.,\n",
    "        color_threshold=0,\n",
    "        labels=data.index.values)\n",
    "    label_color = {str(idx): class_color[class_labels[idx]] for idx in data.index.values}\n",
    "    ax = plt.gca()\n",
    "    xlabels = ax.get_xmajorticklabels()\n",
    "    for label in xlabels:\n",
    "        label.set_color(label_color[label.get_text()])\n",
    "    plt.show()\n",
    "    return merge_dist\n",
    "\n",
    "\n",
    "def facet_plot(subset, class_assignments, facets, class_color, title, suptitle, legends):\n",
    "    categories = class_assignments.unique()\n",
    "    number_classes = len(categories)\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    for i in range(facets):\n",
    "        plt.subplot(1, facets, i+1)\n",
    "        handlers = []\n",
    "        for j in categories:\n",
    "            idx = class_assignments[class_assignments==j].index.values\n",
    "            handlers.append(plt.scatter(idx, subset.ix[idx,i], color=class_color[j]))\n",
    "        plt.title(title[i])\n",
    "        plt.xlabel('Obs Index')\n",
    "        plt.xticks(rotation=45)\n",
    "    plt.suptitle(suptitle, size=16, y=1.1)\n",
    "    fig.legend(handlers, legends, loc='upper center', scatterpoints=1,\n",
    "               ncol=number_classes, bbox_to_anchor=(0.425,0.14))\n",
    "    fig.subplots_adjust(bottom=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activity = {key: value for key, value in get_key_value_pairs('./Data/UCI HAR Dataset/activity_labels.txt')}\n",
    "act_color = {1:'black', 2:'gold', 3:'magenta', 4:'red', 5:'green', 6:'blue'}\n",
    "features = [line.strip().split(' ')[1] for line in open('./Data/UCI HAR Dataset/features.txt', 'r')]\n",
    "\n",
    "Xtrain = pd.read_table('./Data/UCI HAR Dataset/train/X_train.txt', header=None, delim_whitespace=True, names=features)\n",
    "Xtrain['subject'] = [line.strip() for line in open('./Data/UCI HAR Dataset/train/subject_train.txt','r')]\n",
    "Xtrain['activity'] = pd.read_table('./Data/UCI HAR Dataset/train/y_train.txt', header=None, delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the row count by activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pd.crosstab(index=Xtrain['activity'].apply(lambda x: activity[str(x)]), columns='count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject 1\n",
    "Provided that this analysis is exploratory in nature, focus on Subject 1 to isolate the inter-subject variability (name this subset explore). Then, show the importance of selecting the right features as input for HCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Discriminative Features\n",
    "Usually not all features are equally good at separating categories. See an example of an uninformative set of features:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "act_legends = ('Walk','Walk up','Walk down','Sit','Stand','Lay')\n",
    "columns = slice(0,3)\n",
    "facet_plot(explore.iloc[:,columns], explore.activity, 3, act_color, features[columns],\n",
    "           'Tri-axial Mean Body Acc', act_legends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that this set of features will not help us differentiate between activities, as all of them vary on approximately the same range of values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = HCA(explore.iloc[:,columns], explore.activity, act_color)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As foreseen, HCA was not able to find good clusters provided that ID-label colors are scrambled without a clear pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative Features\n",
    "To contrast our previous findings, choose a different set, namely the Tri-axial Max Body Acceleration (columns 9 through 11), and plot the corresponding values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the distinction between the subset {Sitting, Standing,Lying} and the subset {Walking, Walking Upstairs, Walking Downstairs} is obvious, particularly on the X-axis. If intuition is correct, HCA should now be able to at least differentiate these activity subsets, so test your hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, HCA confirms your findings in the previous plots by making the distinction between static and dynamic activities. Nonetheless, it may be harder to discriminate between activities belonging to the same subset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy\n",
    "After exploring some of the features available, you realize that it is relatively easy to discriminate between dynamic activities {Walking, Walking Upstairs, Walking Downstairs} and static activities {Sitting, Standing,Lying}.\n",
    "\n",
    "Thus, you will explore a hierarchical strategy, in that you will first determine whether an activity is dynamic or static, and then you will try to identify such activity. To aid in this endeavor, you will apply PCA to identify Principal Components (PCs) that help separate activities belonging to the same set to further examine which features contribute the most in the specification of such PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore_stat = explore[explore.activity.isin([4,5,6])]\n",
    "explore_stat_col = [act_color[explore_stat.activity[idx]] for idx in explore_stat.index.values]\n",
    "\n",
    "explore_dyn = explore[explore.activity.isin([1,2,3])]\n",
    "explore_dyn_col = [act_color[explore_dyn.activity[idx]] for idx in explore_dyn.index.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on Dynamic Activities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_dyn = PCA(n_components=20, svd_solver='full')\n",
    "scores_dyn = pca_dyn.fit_transform(explore_dyn.iloc[:,:561])\n",
    "scores_dyn = pd.DataFrame(scores_dyn, index=explore_dyn.index.values)\n",
    "print('Total variance explained by the first 20 PCs: {}'.format(np.round(sum(pca_dyn.explained_variance_ratio_), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = ['PC {} scores'.format(i) for i in range(1,5)]\n",
    "act_legends = ('Walk','Walk up','Walk down')\n",
    "facet_plot(scores_dyn.iloc[:,0:4], explore_dyn.activity, 4, act_color, titles, 'First Few PC Scores', act_legends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at PC1 scores, it is notorious how Walking Upstairs (golden color) departs from the other activities by showing positive values. On the other hand, you can see that Walking (black color) is somewhat separated from the rest on PC2 by taking negative scores.\n",
    "\n",
    "You will now analyze which features contribute the most to the score compositions by finding the maximum absolute loadings of PC1 & PC2, respectively. Then you will apply HCA on the max contributors to see whether you can correctly cluster the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row = slice(0,1)\n",
    "max_load_pc1_dyn = np.argmax(np.abs(pca_dyn.components_[row]))\n",
    "print(features[max_load_pc1_dyn])\n",
    "plt.scatter(np.arange(len(features)),pca_dyn.components_[row])\n",
    "plt.title('PC1 loadings for dynamic activities')\n",
    "plt.xlabel('Feature index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row = slice(1,2)\n",
    "max_load_pc2_dyn = np.argmax(np.abs(pca_dyn.components_[row,:]))\n",
    "print(features[max_load_pc2_dyn])\n",
    "plt.scatter(np.arange(len(features)),pca_dyn.components_[row,:])\n",
    "plt.title('PC2 loadings for dynamic activities')\n",
    "plt.xlabel('Feature index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sel_feat_dyn = [max_load_pc1_dyn] + [max_load_pc2_dyn]\n",
    "_ = HCA(explore_dyn.iloc[:,sel_feat_dyn], explore_dyn.activity, act_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now HCA effectively clusters the majority of the 197 observations, incurring in just 15 errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on Static Activities\n",
    "Replicate the previous analysis for the static activities to see which PCs help in the separation of classes. First, apply PCA on the corresponding subset and report the total variance explained by the first 20 PCs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the scores of the first 4 PCs. Which ones help you the most to separate the classes? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at PC2 scores, Lying (blue color) separates from the other activities by showing positive values. On the other hand, you can see that Sitting (red color) tries to depart from the rest on PC4 by taking negative scores (although Lying overlaps a bit).\n",
    "\n",
    "Analyze which features contribute the most to the score compositions by finding the maximum absolute loadings of PC2 and PC4, respectively. Then, apply HCA on the max contributors to see whether you can correctly cluster the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once more, HCA effectively clusters the majority of the 150 observations, which incur in 16 errors.\n",
    "From the exploratory analysis performed, you can conclude that you might build a hierarchical classifier where:\n",
    "1. A distinction is made between static and dynamic activities.\n",
    "\n",
    "    **Note:** Rely on features such as `tBodyAcc-max()-X`.\n",
    "2. Then if an activity has been regarded as:\n",
    "    1. Dynamic, you can employ `fBodyGyro-kurtosis()-Y` and `fBodyGyro-maxInds-X` to identify it.\n",
    "    2. Static, you can employ `tGravityAcc-energy()-X` and `tGravityAcc-correlation()-X,Y` to identify it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Model\n",
    "So far you have employed unsupervised learning for analyzing the dataset at hand. Now you want to learn a descriptive model in a supervised way, specifically a classification tree with the Gini index as the criterion to be minimized. The reason of this is two-fold:\n",
    "- Your exploratory analysis suggested a hierarchical approach, which is intimately related to tree data structures.\n",
    "- While trees do not have a high predictive power, they provide an easy way to understand patterns in a dataset.\n",
    "\n",
    "Before building your classification tree, please note that the sklearn implementation does not support cost-complexity pruning. Therefore, you will manually control its complexity by specifying a minimum number of observations per leaf.\n",
    "\n",
    "Select the five features suggested by our exploratory analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sel_feat = [9] + sel_feat_dyn + sel_feat_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a cart object (cart_tot), fit the data and make prediction about class memberships (pred_tot). You may refer to:\n",
    "- API description and specifications: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "- Examples for how to use the sklearn API: http://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Accuracy: {}'\n",
    "      .format(np.round(accuracy_score(y_true=explore.activity, y_pred=pred_tot, normalize=True), 4)))\n",
    "confusion_matrix(y_true=explore.activity, y_pred=pred_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, visualize the tree model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(cart_tot, out_file=None, \n",
    "                         feature_names=[features[el] for el in sel_feat],  \n",
    "                         class_names=[activity[str(el)] for el in [1,2,3,4,5,6]]\n",
    "                         )  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    "You were able to hack a dataset with hundreds of variables and derive a fairly simple descriptive model, relying on just five features to provide 93% accuracy. Notwithstanding, you have to bear in mind some limitations:\n",
    "1. This analysis was performed with descriptive purposes, so the output model should not be extrapolated on unseen observations.\n",
    "2. If a predictive model were to be built using Subject 1, it is not guaranteed to perform well when classifying activities done by different subjects.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

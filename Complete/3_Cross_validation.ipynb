{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3\n",
    "\n",
    "## Overview\n",
    "\n",
    "At the end of the module, you will use cross-validation to select the right **hyperparameters** for a decision tree classifier.\n",
    "\n",
    "You need to compare results of your classifier and tune it correctly based on error measurements.\n",
    "\n",
    "This module assumes you:\n",
    "\n",
    "* Have a goal\n",
    "* Have a clean data set (already provided and imported)\n",
    "* Have a model ([decision tree classifier](https://en.wikipedia.org/wiki/Decision_tree_learning) in this case)\n",
    "\n",
    "The **goal** of the created model is to classify accurately which human activity is performed depending on the measurements (information from the sensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cross_functions as cf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.tree import DecisionTreeClassifier as cart, export_graphviz\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate the previous data set and selected features\n",
    "\n",
    "The next two code lines recover the features used for the decision tree classifier `sel_feat`, and the training dataset used `explore`. Remember to focus on Subject 1 to remove the inter-subject variability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sel_feat = [9, 457, 448, 56, 77]\n",
    "explore = cf.Xtrain[cf.Xtrain.subject == '1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree validation\n",
    "\n",
    "`sel_feat` is the output of an unsupervised learning method ([PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)) to the dataset. Use validation in the resulting descriptive model ([decision tree classifier](https://en.wikipedia.org/wiki/Decision_tree_learning)) to:\n",
    "\n",
    "- Evaluate if model the behaves accurately in an un-known environment.\n",
    "- Generate solutions for overfitting in case it happens.\n",
    "\n",
    "\n",
    "To do this, you have to:\n",
    "1. **Sample data:** Separate the dataset by using to obtain a sample of **20%** of the data\n",
    "2. **Train model:** train the decision tree model with the remaining **80%**\n",
    "3. **Test accuracy:**: perform accuracy tests to observe it's performance.\n",
    "4. **Cross-validate**: to achieve the two previously mentioned goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sampling\n",
    "\n",
    "Create a `validation set` from the data. Remember to remove the validation set (`test_x` and `test_y`) from the rest of the data (`train_x` and `train_y`).\n",
    "\n",
    "\n",
    "**Random Sampling** is the best sampling approach for obtaining the validation set since this is a classification problem.\n",
    "\n",
    "\n",
    "`sklearn` is a module in python that already has methods for this task. Use the function `train_test_split` as follows:\n",
    "\n",
    "```\n",
    "train_x, test_x, train_y, test_y = train_test_split(DATA_FEATURES, DATA_LABELS, \n",
    "                                                    test_size=RULE_OF_THUMB, random_state=11)\n",
    "\n",
    "```\n",
    "**Note:** As the data set is only for the subject 1, you can not use the test set provided in the HAR data files zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = \\\n",
    "    train_test_split(explore.iloc[:,sel_feat], explore.activity, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test sample should be smaller (around one-fourth) of our train sample, run the following cell to verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('train sample size - rows: %s, columns: %s' % train_x.shape)\n",
    "print('test  sample size - rows: %s,  columns: %s' % test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "After separating the dataset, train the model with the remaining 80%.\n",
    "\n",
    "For the training use the following snippet of code as a base. This code snippet is a modification of the code from the previous session, only now you need to use `train_x` and `train_y`. \n",
    "\n",
    "```\n",
    "cart_tot = cart(min_samples_leaf=39)\n",
    "cart_tot = cart_tot.fit(TRAIN_DATA, TRAIN_ANSWERS)\n",
    "pred_tot = cart_tot.predict(TEST_DATA)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cart_tot = cart(min_samples_leaf=39)\n",
    "cart_tot = cart_tot.fit(train_x, train_y)\n",
    "pred_tot = cart_tot.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy testing\n",
    "Run the following cell to know the trained model's accuracy. Note that it evaluates the accuracy using the test sets `test_y` and `test_x`. You can say you have **validated** the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf.print_accuracy(test_y, pred_tot)\n",
    "graph = cf.print_tree_graph(cart_tot, cf.features, cf.activity, sel_feat)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outcome, as expected, is lower than if you evaluated the accuracy with the training datasets. Nevertheless, using the test datasets gives a better approximation of how the model behaves with new observations.\n",
    "\n",
    "By testing the model on all available data without training from it at the same time, you can improve the out-of-sample error representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "**Cross-validation** is a technique that can accomplish this.\n",
    "\n",
    "Cross validation does not allow for random samples to mix observation between them, so pay extra attention to keep the random samples disjunctive.\n",
    "\n",
    "### Cross-validation sampling\n",
    "\n",
    "Use the function; `compute_disjunctive_random_splits` to do this. This implemented function is very similar to `train_test_split`, except it iterates over the numbers of times you want to split the data frame, and returns for each iteration the result of `train_test_split` for different disjunctive random samples.\n",
    "\n",
    "Run the following cell to look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "masks = cf.compute_disjunctive_random_splits(explore.iloc[:,sel_feat], explore.activity, 10)\n",
    "mask = 0\n",
    "print('numbers of `masks`: %s\\n' % len(masks))\n",
    "print('shape of `train_x` inside layer #%s of `masks`: %s' % (mask, masks[mask][0].shape))\n",
    "print('shape of `test_x`  inside layer #%s of `masks`: %s' % (mask, masks[mask][1].shape))\n",
    "print('shape of `train_y` inside layer #%s of `masks`: %s'% (mask, masks[mask][2].shape))\n",
    "print('shape of `test_y`  inside layer #%s of `masks`: %s'% (mask, masks[mask][3].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation execution\n",
    "\n",
    "Now build a way to re-sample the data set multiple times with different random orders:\n",
    "\n",
    "**Constrains**\n",
    "\n",
    "- Use a different random sample for both training and testing during each iteration.\n",
    "- Ensure test samples do not contain observations used for training.\n",
    "- Use 10% for validation testing as a Rule-of-thumb for the test-train ratio.\n",
    "- Keep random samples disjunctive\n",
    "\n",
    "```\n",
    "# declaration of global variables\n",
    "\n",
    "\n",
    "for iteration in range(0, 10):\n",
    "    # data sampling\n",
    "    \n",
    "    # Model definition, fitting, and prediction\n",
    "    \n",
    "    accuracy = np.round(accuracy_score(y_true=test_y, y_pred=pred_tot, normalize=True), 4)\n",
    "    print('Accuracy for iteration #%s: %s' % (iteration, accuracy))\n",
    "    global_accuracy.append(accuracy)\n",
    "\n",
    "print('Global accuracy #%s' % np.mean(global_accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_accuracy = []\n",
    "\n",
    "masks = cf.compute_disjunctive_random_splits(explore.iloc[:,sel_feat], explore.activity, 10)\n",
    "\n",
    "for iteration in range(0, 10):\n",
    "    train_x, test_x, train_y, test_y = masks[iteration]\n",
    "    \n",
    "    cart_tot = cart(min_samples_leaf=39)\n",
    "    cart_tot = cart_tot.fit(train_x, train_y)\n",
    "    pred_tot = cart_tot.predict(test_x)\n",
    "    \n",
    "    accuracy = np.round(accuracy_score(y_true=test_y, y_pred=pred_tot, normalize=True), 4)\n",
    "    print('Accuracy for iteration #%s: %s' % (iteration, accuracy))\n",
    "    global_accuracy.append(accuracy)\n",
    "\n",
    "print('Global accuracy: %s' % np.mean(global_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the use of Cross validation\n",
    "\n",
    "The model,  `DecisionTreeClassifier` has many **hyperparameter**s to pick on.\n",
    "\n",
    "Look at the [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) function. The followings are some hypterparameters you are using and can modify:\n",
    "\n",
    "- **min_samples_leaf**: already being used, for manual pruning.\n",
    "- **criterion**: The function to measure the quality of a split. Supported methods are “**gini**” for the Gini impurity and “**entropy**” for the information gain.\n",
    "- **max_features**: maximum number of features to consider in each split, \n",
    "\n",
    "Since cross-validation helps us to compare subtle differences between models and test which one is better, the next step is to tune and test the best combination of hyperparameters.\n",
    "\n",
    "`sklearn` once again has made thins easier; the function `ParameterGrid` creates a list of all possible combinations you input, to provide a way to iterate thru said combinations.\n",
    "\n",
    "Enhance the last experiment using `ParameterGrid` to fit `DecisionTreeClassifier` with a new set of **hyperparameters** in each set of iterations.\n",
    "\n",
    "**Constrains**\n",
    "\n",
    "- Use multiple sets of iterations with different random samples for both training and testing.\n",
    "- Ensure test samples do not contain observations used for training.\n",
    "- Use a 10% for validation testing as a Rule-of-thumb for the test-train ratio.\n",
    "- Repeat the combination of hyperparameters for their own cross valisation **exclusively**.\n",
    "- Store all the performance outputs of each combination.\n",
    "- Compare the performance output means for each combination.\n",
    "- Print the set of hyperparameters wit the best performanceBest.\n",
    "- Set **min_samples_leaf** to: `39, 13`\n",
    "- Set **criterion** to: `'gini', 'entropy'`\n",
    "- Set **max_features** to: `5, 4`\n",
    "\n",
    "```\n",
    "# declaration of global variables\n",
    "hyperparameters = [{'max_features': [SET],\n",
    "                    'min_samples_leaf': [SET],\n",
    "                    'criterion': [SET]}]\n",
    "\n",
    "\n",
    "for params in ParameterGrid(hyperparameters): \n",
    "    # declaration of local variables\n",
    "\n",
    "    for iteration in range(0, 10):\n",
    "        # data sampling\n",
    "\n",
    "        # Model definition, fitting, and prediction\n",
    "        cart_tot = cart(**params)\n",
    "        cart_tot = cart_tot.fit(TRAIN_DATA, TRAIN_ANSWERS)\n",
    "        pred_tot = cart_tot.predict(TEST_DATA)\n",
    "\n",
    "        accuracy = np.round(accuracy_score(y_true=test_y, y_pred=pred_tot, normalize=True), 4)\n",
    "        local_accuracy.append(accuracy)\n",
    "    \n",
    "    global_accuracy.append(np.mean(local_accuracy))\n",
    "    print('local accuracy for combination %s: #%s' % (params, np.mean(local_accuracy)))\n",
    "\n",
    "print(ParameterGrid(hyperparameters)[global_accuracy.index(min(global_accuracy))])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# declaration of global variables\n",
    "hyperparameters = [{'max_features': [5, 4],\n",
    "                    'min_samples_leaf': [39, 13],\n",
    "                    'criterion': ['gini', 'entropy']}]\n",
    "global_accuracy = []\n",
    "masks = cf.compute_disjunctive_random_splits(explore.iloc[:,sel_feat], explore.activity, 10)\n",
    "\n",
    "for params in ParameterGrid(hyperparameters): \n",
    "    # declaration of local variables\n",
    "    local_accuracy = []\n",
    "    for iteration in range(0, 10):\n",
    "        # data sampling\n",
    "        train_x, test_x, train_y, test_y = masks[iteration]\n",
    "        \n",
    "        # Model definition, fitting, and prediction\n",
    "        cart_tot = cart(**params)\n",
    "        cart_tot = cart_tot.fit(train_x, train_y)\n",
    "        pred_tot = cart_tot.predict(test_x)\n",
    "\n",
    "        accuracy = np.round(accuracy_score(y_true=test_y, y_pred=pred_tot, normalize=True), 4)\n",
    "        local_accuracy.append(accuracy)\n",
    "    \n",
    "    global_accuracy.append(np.mean(local_accuracy))\n",
    "    print('local accuracy for combination %s: #%s' % (params, np.mean(local_accuracy)))\n",
    "\n",
    "print('\\n\\nBest combination: %s' % ParameterGrid(hyperparameters)[global_accuracy.index(max(global_accuracy))] + \n",
    "      '\\nWith an average accuracy of: %s' % max(global_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "You satisfactorily performed hyperparameters tunning for a model using cross-validation, thus avoiding to use any rule of thumb in said hyperparameters, and solving a common and non-trivial problem in a day-to-day classification problem.\n",
    "\n",
    "Regardless, you must remember:\n",
    "\n",
    "1. This is still a descriptive analysis, for all combinations to be tested, a parameter grid of much higher dimensions should be used.\n",
    "2. You used only subject 1, hence it is not guaranteed that any model built using the restricted data will perform well when classifying activities done by different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
